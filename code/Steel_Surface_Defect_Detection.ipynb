{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "import imgaug.augmenters as iaa\n",
    "import segmentation_models as sm\n",
    "\n",
    "from PIL import Image \n",
    "from segmentation_models import Unet\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sm.set_framework('tf.keras')\n",
    "sm.framework()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdZrvemeg4hz"
   },
   "outputs": [],
   "source": [
    "\"\"\" 데이터의 경로 지정 \"\"\"\n",
    "\n",
    "train_images_path = '../input/severstal-steel-defect-detection/train_images'\n",
    "train_df = pd.read_csv('../input/severstal-steel-defect-detection/train.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 훈련용 이미지의 이름 목록 저장\"\"\"\n",
    "train_image_names = os.listdir(train_images_path)\n",
    "train_image_names = pd.DataFrame(train_image_names, columns =['ImageId'])\n",
    "train_image_names # train 폴더에는 12568개의 이미지가 존재함을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "L2WxYAUDjYRf",
    "outputId": "02ce2a21-f603-4edf-f92d-b467456c1249"
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "pandas의 merge를 이용하여 두 데이터프레임 train.csv와 train image 목록을 ImageId를 기준으로 병합\n",
    "7095개의 결함과 결함이 없는 5902개의 이미지를 결합하여 결과적으로 12997개의 행이 됨\n",
    "\"\"\"\n",
    "train_df = pd.merge(train_df,train_image_names,how = 'outer',on = ['ImageId','ImageId'])\n",
    "train_df = train_df.fillna(' ')\n",
    "train_df # 12997(7095+5902) x 3 (ImageId, ClassId, EncodedPixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "id": "DkEDeK_SkoTx",
    "outputId": "2c2a7bd2-ee08-42a4-d5c9-827136c6dab6"
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "ImageId를 행 인덱스로, ClassId를 열 인덱스로, EncodedPixels를 분석할 열로 지정\n",
    "numpy 합을 이용하되, 없는 값은 공백인 ' '로 채움\n",
    "\"\"\"\n",
    "train_data = pd.pivot_table(train_df, values='EncodedPixels', index='ImageId',columns='ClassId', aggfunc=np.sum,fill_value= ' ').astype(str)\n",
    "train_data # 이미지별 결함의 위치로 변경하며 12568개로 돌아옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" train_df에는 ClassId가 공백인, 결함이 없는 이미지가 있었으므로 공백 열이 생성됨 \"\"\"\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 결함이 없으므로 아무런 데이터가 없는 열임 \"\"\"\n",
    "train_data[' '].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 삭제해줌 \"\"\"\n",
    "del train_data[' ']\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "index 설정을 리셋하여 ClassId 설정을 지운다.\n",
    "\"\"\"\n",
    "train_data = train_data.reset_index()\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ClasssId 1, 2, 3, 4를 결함 1, 2, 3, 4로 수정 \"\"\"\n",
    "train_data.columns = ['ImageId','Defect_1','Defect_2','Defect_3','Defect_4']\n",
    "train_data # Defect 1, 2, 3, 4의 위치를 12568개의 이미지에 각각 표시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "0j7o1xzrhNq8",
    "outputId": "d63b9739-89e1-44f0-9def-e58d64e44170"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "각각의 이미지가 결함을 가지고 있는지 알려주는 hasDefect, 결함 1, 2, 3, 4를 가지고 있는지를 알려주는 hasDefect_1/2/3/4 열 생성\n",
    "\"\"\"\n",
    "\n",
    "insert_column = []\n",
    "for i in range(len(train_data)):\n",
    "    if (train_data['Defect_1'][i]== ' ' and train_data['Defect_2'][i]== ' ' and train_data['Defect_3'][i]==' ' and train_data['Defect_4'][i]==' '):\n",
    "        insert_column.append(0)\n",
    "    else:\n",
    "        insert_column.append(1)\n",
    "train_data['hasDefect'] = insert_column\n",
    "\n",
    "insert_column = []\n",
    "for i in range(len(train_data)):\n",
    "    if train_data['Defect_1'][i]==' ':\n",
    "        insert_column.append(0)\n",
    "    else:\n",
    "        insert_column.append(1)\n",
    "train_data['hasDefect_1'] = insert_column\n",
    "\n",
    "insert_column = []\n",
    "for i in range(len(train_data)):\n",
    "    if train_data['Defect_2'][i]==' ':\n",
    "        insert_column.append(0)\n",
    "    else:\n",
    "        insert_column.append(1)\n",
    "train_data['hasDefect_2'] = insert_column\n",
    "\n",
    "insert_column = []\n",
    "for i in range(len(train_data)):\n",
    "    if train_data['Defect_3'][i]==' ':\n",
    "        insert_column.append(0)\n",
    "    else:\n",
    "        insert_column.append(1)\n",
    "train_data['hasDefect_3'] = insert_column\n",
    "\n",
    "insert_column = []\n",
    "for i in range(len(train_data)):\n",
    "    if train_data['Defect_4'][i]==' ':\n",
    "        insert_column.append(0)\n",
    "    else:\n",
    "        insert_column.append(1)\n",
    "train_data['hasDefect_4'] = insert_column\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (4.8, 4)) \n",
    "plt.xlabel('number of defects in each image')\n",
    "plt.ylabel('Number')\n",
    "plt.bar([str(0),str(1),str(2),str(3)], train_data[['hasDefect_1','hasDefect_2','hasDefect_3','hasDefect_4']].sum(axis = 1).value_counts().sort_index(),\n",
    "        color = ['orange','blue','green','red'] , width = 0.8)\n",
    "plt.title('Distribution of number of defects in each image')\n",
    "\n",
    "xlocs, xlabs = plt.xticks()\n",
    "for i, v in zip( [0,1,2,3], train_data[['hasDefect_1','hasDefect_2','hasDefect_3','hasDefect_4']].sum(axis = 1).value_counts().sort_index()):\n",
    "    plt.text(xlocs[i] - 0.15, v + 0.5, str(v))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (4.8, 4)) \n",
    "plt.xlabel('Defect type')\n",
    "plt.ylabel('Number')\n",
    "plt.bar([1,2,3,4],train_data[['hasDefect_1','hasDefect_2','hasDefect_3','hasDefect_4']].sum(axis = 0),\n",
    "        color = ['red','green','blue','orange'] , width = 0.8)\n",
    "xlocs, xlabs = plt.xticks()\n",
    "plt.title('Distribution of steel among defect types')\n",
    "for i, v in zip([1,2,3,4],train_data[['hasDefect_1','hasDefect_2','hasDefect_3','hasDefect_4']].sum(axis = 0)):\n",
    "    plt.text(xlocs[i] - 0.15, v + 0.5, str(v))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**분석 결과 대다수의 이미지는 결함이 한 개이나, 다중 결함을 갖는 결함이 존재.  \n",
    "또한 데이터의 분포가 매우 불균형하여, 증강이 필요**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "id": "Ht8pELEXHBxl",
    "outputId": "64492200-6c24-4280-9e1a-d4322816b545"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "minority preference는 데이터가 소수인 클래스를 의미\n",
    "소수 레이블을 우선적으로 사용하여 다중 결함 클래스에서 2, 4, 1, 3 순으로 데이터를 확인\n",
    "\"\"\"\n",
    "\n",
    "insert_column = []\n",
    "for i in range(len(train_data)):\n",
    "    if train_data['hasDefect_2'].iloc[i]==1:\n",
    "        insert_column.append(2)\n",
    "    elif train_data['hasDefect_4'].iloc[i]==1:\n",
    "        insert_column.append(4)\n",
    "    elif train_data['hasDefect_1'].iloc[i]==1:\n",
    "        insert_column.append(1)\n",
    "    elif train_data['hasDefect_3'].iloc[i]==1:\n",
    "        insert_column.append(3)\n",
    "    else:\n",
    "        insert_column.append(0)\n",
    "        \n",
    "train_data['minority_preference']= insert_column\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "450uLB6pKUd2",
    "outputId": "643bddd6-7c43-4bb4-b612-e1320e6e43ba"
   },
   "outputs": [],
   "source": [
    "X = train_data.copy()\n",
    "X_train, X_test = train_test_split(X, test_size = 0.2, stratify = X['minority_preference'],random_state=2022)\n",
    "X_train, X_validation = train_test_split(X_train, test_size = 0.25, stratify = X_train['minority_preference'],random_state=2022)\n",
    "\n",
    "print(X_train.shape, X_validation.shape, X_test.shape) # train, validation, test를 대략 6:2:2로 분리 (7540:2514:2514)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('../input/severstal-steel-defect-detection/X_train.csv')\n",
    "X_validation.to_csv('../input/severstal-steel-defect-detection/X_validation.csv')\n",
    "X_test.to_csv('../input/severstal-steel-defect-detection/X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X\n",
    "del train_data\n",
    "del train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rle2maskResize(rle):\n",
    "    \"\"\" RLE 형식으로 코딩된 Encodedpixels를 mask 형태로 분리 \"\"\"\n",
    "    if (pd.isnull(rle))|(rle==''): \n",
    "        return np.zeros((height,width) ,dtype=np.uint8)\n",
    "    \n",
    "    mask= np.zeros( 256 * 1600 ,dtype=np.uint8)\n",
    "\n",
    "    array = np.asarray([int(x) for x in rle.split()])\n",
    "    starts = array[0::2]-1\n",
    "    lengths = array[1::2]    \n",
    "    for index, start in enumerate(starts):\n",
    "        mask[int(start):int(start+lengths[index])] = 1\n",
    "    \n",
    "    if height == 128 and width == 800:\n",
    "        return mask.reshape( (256, 1600), order='F' )[::2,::2]\n",
    "    return mask.reshape( (256, 1600), order='F' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" keras를 이용하여 커스텀 데이터로더 생성 \"\"\"\n",
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, df, batch_size = 16, subset=\"train\", shuffle=False, preprocess=None, augmentation=None, info={}):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.shuffle = shuffle\n",
    "        self.subset = subset\n",
    "        self.batch_size = batch_size\n",
    "        self.preprocess = preprocess\n",
    "        self.augmentation = augmentation\n",
    "        self.info = info\n",
    "        self.path = '../input/severstal-steel-defect-detection/'\n",
    "        self.data_path = self.path + 'train_images/'\n",
    "\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.df) / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.df))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        X = np.empty((self.batch_size, height, width, 3)).astype(np.float16)\n",
    "        y = np.empty((self.batch_size, height, width, 4)).astype(np.float16)\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        for i,f in enumerate(self.df['ImageId'].iloc[indexes]):\n",
    "            self.info[index*self.batch_size+i]=f\n",
    "            if height == 128 and width == 800:\n",
    "                X[i,] = Image.open(self.data_path + f).resize((800,128))\n",
    "            else:\n",
    "                X[i,] = Image.open(self.data_path + f)\n",
    "            for j in range(4):\n",
    "                y[i,:,:,j] = rle2maskResize(self.df['Defect_'+str(j+1)].iloc[indexes[i]])\n",
    "        \n",
    "        if self.augmentation != None and self.subset == 'train': # 증강은 train일때만, 입력했다면 적용\n",
    "            for train_image in X:\n",
    "                train_image = self.augmentation(train_image)\n",
    "                \n",
    "            for train_label in y:\n",
    "                train_label = self.augmentation(train_label)\n",
    "\n",
    "        if self.preprocess != None: # 전처리를 입력했다면 적용\n",
    "            X = self.preprocess(X)\n",
    "            \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 전처리 \"\"\"\n",
    "def rescaling(item): # 값 범위를 0~255(pixel)에서 0~1로 정규화\n",
    "    return item * 1./255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" imgaug 라이브러리를 이용한 이미지 데이터 증강 \"\"\"\n",
    "def flip_augmentation(item):\n",
    "    aug2 = iaa.Fliplr(1) # parameter * 180 도 만큼 이미지를 수평으로 뒤집음, 여기서는 좌우 대칭이 됨\n",
    "    aug3 = iaa.Flipud(1) # parameter * 180 도 만큼 이미지를 수직으로 뒤집음, 여기서는 상하 대칭이 됨\n",
    "\n",
    "    a = np.random.uniform()\n",
    "    if a < 0.33:\n",
    "        item = aug2.augment_image(item)\n",
    "    elif a < 0.66:\n",
    "        item = aug3.augment_image(item)\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/bigironsphere/loss-function-library-keras-pytorch\n",
    "def dice_coef(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def focal_tversky_loss(targets, inputs, alpha=0.3, beta=0.7, gamma=0.75, smooth=1e-6): \n",
    "    #flatten label and prediction tensors\n",
    "    inputs = K.flatten(inputs)\n",
    "    targets = K.flatten(targets)\n",
    "\n",
    "    #True Positives, False Positives & False Negatives\n",
    "    TP = K.sum((inputs * targets))\n",
    "    FP = K.sum(((1-targets) * inputs))\n",
    "    FN = K.sum((targets * (1-inputs)))\n",
    "               \n",
    "    Tversky = (TP + smooth) / (TP + alpha*FP + beta*FN + smooth)  \n",
    "    FocalTversky = K.pow((1 - Tversky), gamma)\n",
    "        \n",
    "    return FocalTversky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "epochs = 30\n",
    "height = 128\n",
    "width = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset =      DataGenerator(X_train, batch_size= BATCH_SIZE, subset = \"train\", shuffle=True,\n",
    "                            preprocess = rescaling, augmentation = flip_augmentation)\n",
    "\n",
    "validation_dataset = DataGenerator(X_validation, batch_size= BATCH_SIZE, subset = 'validation',\n",
    "                            preprocess = rescaling)\n",
    "\n",
    "test_dataset =       DataGenerator(X_test, batch_size= BATCH_SIZE, subset = 'test',\n",
    "                            preprocess = rescaling)\n",
    "\n",
    "assert train_dataset[0][0].shape == (BATCH_SIZE, height, width, 3)\n",
    "assert validation_dataset[0][0].shape == (BATCH_SIZE, height, width, 3)\n",
    "assert test_dataset[0][0].shape == (BATCH_SIZE, height, width, 3)\n",
    "\n",
    "assert train_dataset[0][1].shape == (BATCH_SIZE, height, width, 4)\n",
    "assert validation_dataset[0][1].shape == (BATCH_SIZE, height, width, 4)\n",
    "assert test_dataset[0][1].shape == (BATCH_SIZE, height, width, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet('resnet18', input_shape=(height, width, 3), classes=4, activation='sigmoid') # segmentation_model.Unet Unet\n",
    "model.compile(optimizer='adam', loss = focal_tversky_loss, metrics=[dice_coef])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\" validation loss가 최소인 가중치를 저장 \"\"\"\n",
    "callback = [ \n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "      'best_weight.h5', \n",
    "      save_weights_only=True, \n",
    "      save_best_only=True,\n",
    "      mode='min', \n",
    "      monitor='val_loss')\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\" 모델 학습 \"\"\"\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs = epochs,\n",
    "    validation_data = validation_dataset,\n",
    "    callbacks = [callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, (loss_plot, dice_plot) = plt.subplots(nrows = 1,ncols = 2,figsize=(25, 6))\n",
    "\n",
    "loss_plot.plot(history.history['loss'], 'r', label = 'train loss')\n",
    "loss_plot.plot(history.history['val_loss'], 'b', label = 'validation loss')\n",
    "loss_plot.legend()\n",
    "loss_plot.set_xlabel('epoch')\n",
    "loss_plot.set_ylabel('loss')\n",
    "\n",
    "\n",
    "dice_plot.plot(history.history['dice_coef'], 'r', label = 'train dice coefficient')\n",
    "dice_plot.plot(history.history['val_dice_coef'], 'b', label = 'validation dice coefficient')\n",
    "dice_plot.legend()\n",
    "dice_plot.set_xlabel('epoch')\n",
    "dice_plot.set_ylabel('dice coefficient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" 테스트 데이터셋으로 평가하여 성능 측정 \"\"\"\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask2pad(mask, pad=2):\n",
    "    \"\"\"\n",
    "    Enlarge Mask to include more space around the defect\n",
    "    \"\"\"\n",
    "    w = mask.shape[1]\n",
    "    h = mask.shape[0]\n",
    "    \n",
    "    # MASK UP\n",
    "    for k in range(1,pad,2):\n",
    "        temp = np.concatenate([mask[k:,:],np.zeros((k,w))],axis=0)\n",
    "        mask = np.logical_or(mask,temp)\n",
    "    # MASK DOWN\n",
    "    for k in range(1,pad,2):\n",
    "        temp = np.concatenate([np.zeros((k,w)),mask[:-k,:]],axis=0)\n",
    "        mask = np.logical_or(mask,temp)\n",
    "    # MASK LEFT\n",
    "    for k in range(1,pad,2):\n",
    "        temp = np.concatenate([mask[:,k:],np.zeros((h,k))],axis=1)\n",
    "        mask = np.logical_or(mask,temp)\n",
    "    # MASK RIGHT\n",
    "    for k in range(1,pad,2):\n",
    "        temp = np.concatenate([np.zeros((h,k)),mask[:,:-k]],axis=1)\n",
    "        mask = np.logical_or(mask,temp)\n",
    "    \n",
    "    return mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask2contour(mask, width=3):\n",
    "    \"\"\"\n",
    "    Convert mask to its contour\n",
    "    \"\"\"\n",
    "    w = mask.shape[1]\n",
    "    h = mask.shape[0]\n",
    "    mask2 = np.concatenate([mask[:,width:],np.zeros((h,width))],axis=1)\n",
    "    mask2 = np.logical_xor(mask,mask2)\n",
    "    mask3 = np.concatenate([mask[width:,:],np.zeros((width,w))],axis=0)\n",
    "    mask3 = np.logical_xor(mask,mask3)\n",
    "    return np.logical_or(mask2,mask3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defects = list(X_test[X_test['Defect_1']!=''].sample(3).index)\n",
    "defects += list(X_test[X_test['Defect_2']!=''].sample(3).index)\n",
    "defects += list(X_test[X_test['Defect_3']!=''].sample(7).index)\n",
    "defects += list(X_test[X_test['Defect_4']!=''].sample(3).index)\n",
    "\n",
    "valid_batches = DataGenerator(X_test[X_test.index.isin(defects)], preprocess = rescaling)\n",
    "preds = model.predict(valid_batches,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Plotting predictions...')\n",
    "print('KEY: yellow=defect1, green=defect2, blue=defect3, magenta=defect4')\n",
    "\n",
    "for i,batch in enumerate(valid_batches):\n",
    "    plt.figure(figsize=(20,36))\n",
    "    for k in range(16):\n",
    "        plt.subplot(16,2,2*k+1)\n",
    "        img = batch[0][k,]\n",
    "        img = Image.fromarray(img.astype('uint8'))\n",
    "        img = np.array(img)\n",
    "        dft = 0\n",
    "        extra = '  has defect '\n",
    "        for j in range(4):\n",
    "            msk = batch[1][k,:,:,j]\n",
    "            if np.sum(msk)!=0:\n",
    "                dft=j+1\n",
    "                extra += ' '+str(j+1)\n",
    "            msk = mask2pad(msk,pad=2)\n",
    "            msk = mask2contour(msk,width=3)\n",
    "            if j==0: # yellow\n",
    "                img[msk==1,0] = 235 \n",
    "                img[msk==1,1] = 235\n",
    "            elif j==1: img[msk==1,1] = 210\n",
    "            elif j==2: img[msk==1,2] = 255\n",
    "            elif j==3: # magenta\n",
    "                img[msk==1,0] = 255\n",
    "                img[msk==1,2] = 255\n",
    "        if extra=='  has defect ': extra =''\n",
    "        plt.title('Train '+ X_test.iloc[16*i+k,0]+extra)\n",
    "        plt.axis('off') \n",
    "        plt.imshow(img)\n",
    "        plt.subplot(16,2,2*k+2)\n",
    "        if dft!=0:\n",
    "            msk = preds[16*i+k,:,:,dft-1]\n",
    "            mx = np.round(np.max(msk),3)\n",
    "            plt.title('Predict Defect '+str(dft)+'  (max pixel = '+str(mx)+')')\n",
    "            plt.imshow(msk, cmap=plt.cm.gray)\n",
    "        else:\n",
    "            plt.title('Predict No Defect')\n",
    "            plt.imshow(np.zeros((height,width)), cmap=plt.cm.gray)\n",
    "        plt.axis('off')\n",
    "    plt.subplots_adjust(wspace=0.05)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "7Xndo45DiU19",
    "xoa5gxdUvl54",
    "FazzHS5hkkbE",
    "RwgQB1NSKHgY",
    "cUyvy49Dj87L",
    "ppyetFZrStwD",
    "cLfUk4jztB_Z",
    "NvHzjGAs4lv3",
    "9mlqalrh8R79",
    "B8JQC8zKhtqK",
    "MRkX4SCMHYo0",
    "f4acrsyehaGU",
    "ZgYv0IlKBOTQ"
   ],
   "machine_shape": "hm",
   "name": "Steel_defect.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
